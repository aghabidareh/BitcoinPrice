{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9f95a-461b-4785-821e-df2aee9eeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2576ad-9923-4cf6-b199-e2be14080756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BTC.csv')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3afb7-0514-4a69-8a29-e9fed1f6f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af16d9-26a3-48c5-b2b8-3defda258613",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c6dae-f36e-4b02-bd7f-e603379d8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b243a8-fb89-4497-8865-64326cbccb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['Close']].values\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320fc29-9a40-47fe-be15-fee51e9bde72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2448e2-6938-43ee-bf6a-14e6f7cbeec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e71049-ca68-4fbc-a1ba-93ff452818d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 60\n",
    "EPISODES = 1000\n",
    "GAMMA = 0.95\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "EPSILON_MIN = 0.01\n",
    "LEARNING_RATE = 0.001\n",
    "MEMORY_SIZE = 2000\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1a953-ff80-4117-9244-7b1fca5bf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_sequences(data, seq_length):\n",
    "    states = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        states.append(data[i:i + seq_length])\n",
    "    return np.array(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331ccfc-7baa-4ffd-895f-e8291355a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = create_state_sequences(data_scaled, SEQ_LENGTH)\n",
    "actions = [\"BUY\", \"SELL\", \"HOLD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11f5ec-2702-416a-ae32-c5fc3f8a52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(SEQ_LENGTH, 1)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(actions), activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715eda2-b6c3-4769-878c-83002178af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.model = build_model()\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= EPSILON:\n",
    "            return random.randint(0, len(actions) - 1)\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += GAMMA * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "\n",
    "            q_values = self.model.predict(state, verbose=0)\n",
    "            q_values[0][action] = target\n",
    "            self.model.fit(state, q_values, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b4322-b4cc-466e-a038-6c95bd572a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3ea4b-ed99-4f33-9967-813916c97ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(EPISODES):\n",
    "    state_idx = 0\n",
    "    total_reward = 0\n",
    "    state = states[state_idx].reshape(1, SEQ_LENGTH, 1)\n",
    "\n",
    "    while state_idx < len(states) - 1:\n",
    "        action = agent.act(state)\n",
    "        next_state = states[state_idx + 1].reshape(1, SEQ_LENGTH, 1)\n",
    "\n",
    "        price_diff = data[state_idx + SEQ_LENGTH] - data[state_idx + SEQ_LENGTH - 1]\n",
    "        if action == 0: \n",
    "            reward = price_diff\n",
    "        elif action == 1: \n",
    "            reward = -price_diff\n",
    "        else: \n",
    "            reward = 0\n",
    "\n",
    "        total_reward += reward\n",
    "        done = state_idx == len(states) - 2\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state_idx += 1\n",
    "        state = next_state\n",
    "\n",
    "    agent.train()\n",
    "    \n",
    "    global EPSILON\n",
    "    if EPSILON > EPSILON_MIN:\n",
    "        EPSILON = float(EPSILON * EPSILON_DECAY)\n",
    "\n",
    "    print(f\"Episode {episode+1}/{EPISODES}, Reward: {total_reward:.2f}, Epsilon: {EPSILON:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8a778-e408-454c-bb34-a7db7a469f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.save(\"bitcoin_rl_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
